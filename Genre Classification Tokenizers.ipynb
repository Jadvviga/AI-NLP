{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28d8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import StripAccents\n",
    "from tokenizers.pre_tokenizers import Sequence, WhitespaceSplit, Punctuation\n",
    "from tokenizers.processors import TemplateProcessing, ByteLevel\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.models import WordLevel, BPE, WordPiece\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5163a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to a certain size, use nRows = None to read all data\n",
    "nrows = 1000\n",
    "\n",
    "# read train data set\n",
    "df_train = pd.read_csv('Genre Classification Dataset/train_data.txt', sep=':::',\\\n",
    "                       engine='python', header=None, nrows=nrows)\n",
    "# rename columns\n",
    "df_train.rename(columns={0:'id', 1:'title', 2:'genre', 3:'description'}, inplace=True)\n",
    "# make everything lower case and remove trailing whitespaces\n",
    "df_train['description'] = df_train['description'].apply(lambda x: x.lower().strip())\n",
    "df_train['genre'] = df_train['genre'].apply(lambda x: x.lower().strip())\n",
    "\n",
    "# read test data set\n",
    "df_test = pd.read_csv('Genre Classification Dataset/test_data_solution.txt', sep=':::',\\\n",
    "                       engine='python', header=None, nrows=nrows)\n",
    "# rename columns\n",
    "df_test.rename(columns={0:'id', 1:'title', 2:'genre', 3:'description'}, inplace=True)\n",
    "# make everything lower case and remove trailing whitespaces\n",
    "df_test['description'] = df_test['description'].apply(lambda x: x.lower().strip())\n",
    "df_test['genre'] = df_test['genre'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a09a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "# train tokenizer\n",
    "trainer = WordLevelTrainer(special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]'],\\\n",
    "                          vocab_size=10000)\n",
    "tokenizer.normalizer = StripAccents() # add method for stripping accents\n",
    "tokenizer.pre_tokenizer = Sequence([WhitespaceSplit(), Punctuation(behavior='removed')]) \n",
    "#tokenizer.post_processor = TemplateProcessing(single='[CLS] $0 [SEP]',\\\n",
    "#                                              special_tokens=[('[CLS]',1),('[SEP]',2)])\n",
    "tokenizer.train_from_iterator(df_train['description'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb048b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 10000\n",
      "vocabulary id: 0, word: [UNK]\n",
      "vocabulary id: 9999, word: gigs\n",
      "vocabulary id: 1, word: [CLS]\n",
      "vocabulary id: 9998, word: gibbs\n",
      "vocabulary id: 2, word: [SEP]\n",
      "vocabulary id: 9997, word: gibbons\n",
      "vocabulary id: 3, word: [PAD]\n",
      "vocabulary id: 9996, word: giants\n",
      "vocabulary id: 4, word: the\n",
      "vocabulary id: 9995, word: giantlands\n",
      "vocabulary id: 5, word: and\n",
      "vocabulary id: 9994, word: gianni\n",
      "vocabulary id: 6, word: a\n",
      "vocabulary id: 9993, word: giaguara\n",
      "vocabulary id: 7, word: of\n",
      "vocabulary id: 9992, word: ghraib\n",
      "vocabulary id: 8, word: to\n",
      "vocabulary id: 9991, word: ghoulish\n",
      "vocabulary id: 9, word: in\n",
      "vocabulary id: 9990, word: ghettos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"l.r. brane loves his life - his car, his apartment, his job, but especially his girlfriend, vespa. one day while showering, vespa runs out of shampoo. l.r. runs across the street to a convenience store to buy some more, a quick trip of no more than a few minutes. when he returns, vespa is gone and every trace of her existence has been wiped out. l.r.'s life becomes a tortured existence as one strange event after another occurs to confirm in his mind that a conspiracy is working against his finding vespa.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[857, 1151, 0, 488, 11, 34, 11, 353, 11, 517, 11, 233, 26, 784, 11, 357, 0, 36, 70, 97, 0, 0, 1450, 45, 7, 0, 857, 1151, 1450, 267, 4, 646, 8, 6, 5255, 1947, 8, 2970, 82, 60, 6, 1906, 435, 7, 103, 60, 109, 6, 312, 1278, 35, 14, 451, 0, 10, 1392, 5, 212, 0, 7, 15, 786, 31, 92, 0, 45, 857, 1151, 12, 34, 158, 6, 6968, 786, 19, 36, 645, 621, 51, 198, 3315, 8, 0, 9, 11, 505, 17, 6, 3836, 10, 341, 182, 11, 2549, 0]\n",
      "['l', 'r', '[UNK]', 'loves', 'his', 'life', 'his', 'car', 'his', 'apartment', 'his', 'job', 'but', 'especially', 'his', 'girlfriend', '[UNK]', 'one', 'day', 'while', '[UNK]', '[UNK]', 'runs', 'out', 'of', '[UNK]', 'l', 'r', 'runs', 'across', 'the', 'street', 'to', 'a', 'convenience', 'store', 'to', 'buy', 'some', 'more', 'a', 'quick', 'trip', 'of', 'no', 'more', 'than', 'a', 'few', 'minutes', 'when', 'he', 'returns', '[UNK]', 'is', 'gone', 'and', 'every', '[UNK]', 'of', 'her', 'existence', 'has', 'been', '[UNK]', 'out', 'l', 'r', 's', 'life', 'becomes', 'a', 'tortured', 'existence', 'as', 'one', 'strange', 'event', 'after', 'another', 'occurs', 'to', '[UNK]', 'in', 'his', 'mind', 'that', 'a', 'conspiracy', 'is', 'working', 'against', 'his', 'finding', '[UNK]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l r loves his life his car his apartment his job but especially his girlfriend one day while runs out of l r runs across the street to a convenience store to buy some more a quick trip of no more than a few minutes when he returns is gone and every of her existence has been out l r s life becomes a tortured existence as one strange event after another occurs to in his mind that a conspiracy is working against his finding'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of unknown tokens: 0.1245\n",
      "total number of tokens: 102719\n"
     ]
    }
   ],
   "source": [
    "vocabSize = tokenizer.get_vocab_size()\n",
    "print('size of vocabulary: {}'.format(vocabSize))\n",
    "for i in range(10):\n",
    "    print('vocabulary id: {0}, word: {1}'.format(i, tokenizer.id_to_token(i)))\n",
    "    j = vocabSize-i-1\n",
    "    print('vocabulary id: {0}, word: {1}'.format(j, tokenizer.id_to_token(j)))\n",
    "    \n",
    "display(df_test.loc[0, 'description'])\n",
    "out=tokenizer.encode(df_test.loc[0,'description'])\n",
    "print(out.ids)\n",
    "print(out.tokens)\n",
    "display(tokenizer.decode(out.ids))\n",
    "\n",
    "outFull=tokenizer.encode_batch(df_test['description'])\n",
    "ntokens = 0\n",
    "nunk = 0\n",
    "for encoded in outFull:\n",
    "    ntokens += len(encoded.ids) \n",
    "    nunk += len(encoded.ids) - np.count_nonzero(encoded.ids)\n",
    "print('ratio of unknown tokens: {0:.4f}'.format(nunk/ntokens))\n",
    "\n",
    "print('total number of tokens: {}'.format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6ba36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerBPE = Tokenizer(BPE(unk_token='[UNK]',dropout=None))\n",
    "# train tokenizer\n",
    "trainer = BpeTrainer(special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]'],\\\n",
    "                     continuing_subword_prefix='##',\\\n",
    "                     vocab_size=10000)\n",
    "tokenizerBPE.normalizer = StripAccents() # add method for stripping accents\n",
    "tokenizerBPE.pre_tokenizer = Sequence([WhitespaceSplit(), Punctuation(behavior='removed')]) \n",
    "tokenizerBPE.post_processor = ByteLevel(trim_offsets=True)\n",
    "tokenizerBPE.train_from_iterator(df_train['description'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dacfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 10000\n",
      "vocabulary id: 0, word: [UNK]\n",
      "vocabulary id: 9999, word: marivela\n",
      "vocabulary id: 1, word: [CLS]\n",
      "vocabulary id: 9998, word: marijuana\n",
      "vocabulary id: 2, word: [SEP]\n",
      "vocabulary id: 9997, word: marginal\n",
      "vocabulary id: 3, word: [PAD]\n",
      "vocabulary id: 9996, word: marisa\n",
      "vocabulary id: 4, word: 0\n",
      "vocabulary id: 9995, word: mario\n",
      "vocabulary id: 5, word: 1\n",
      "vocabulary id: 9994, word: marks\n",
      "vocabulary id: 6, word: 2\n",
      "vocabulary id: 9993, word: marine\n",
      "vocabulary id: 7, word: 3\n",
      "vocabulary id: 9992, word: mari\n",
      "vocabulary id: 8, word: 4\n",
      "vocabulary id: 9991, word: whenever\n",
      "vocabulary id: 9, word: 5\n",
      "vocabulary id: 9990, word: allog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"l.r. brane loves his life - his car, his apartment, his job, but especially his girlfriend, vespa. one day while showering, vespa runs out of shampoo. l.r. runs across the street to a convenience store to buy some more, a quick trip of no more than a few minutes. when he returns, vespa is gone and every trace of her existence has been wiped out. l.r.'s life becomes a tortured existence as one strange event after another occurs to confirm in his mind that a conspiracy is working against his finding vespa.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 31, 268, 1112, 1829, 156, 271, 156, 385, 156, 1964, 156, 980, 237, 2555, 156, 1456, 35, 127, 7396, 298, 395, 587, 419, 1246, 35, 127, 7396, 3638, 299, 138, 231, 543, 2593, 25, 31, 3638, 1209, 123, 2112, 136, 14, 8021, 740, 4227, 136, 4890, 466, 437, 14, 1831, 1405, 138, 583, 437, 603, 14, 1286, 3348, 294, 169, 1730, 35, 127, 7396, 151, 3460, 135, 496, 203, 361, 138, 184, 2538, 255, 574, 36, 283, 140, 299, 25, 31, 32, 271, 806, 14, 3058, 1511, 2538, 189, 298, 2195, 1345, 373, 989, 6727, 136, 6625, 82, 139, 156, 1432, 195, 14, 8719, 151, 1410, 910, 156, 5478, 35, 127, 7396]\n",
      "['l', 'r', 'br', '##ane', 'loves', 'his', 'life', 'his', 'car', 'his', 'apartment', 'his', 'job', 'but', 'especially', 'his', 'girlfriend', 'v', '##es', '##pa', 'one', 'day', 'while', 'show', '##ering', 'v', '##es', '##pa', 'runs', 'out', 'of', 'sh', '##amp', '##oo', 'l', 'r', 'runs', 'across', 'the', 'street', 'to', 'a', 'conven', '##ience', 'store', 'to', 'buy', 'some', 'more', 'a', 'quick', 'trip', 'of', 'no', 'more', 'than', 'a', 'few', 'minutes', 'when', 'he', 'returns', 'v', '##es', '##pa', 'is', 'gone', 'and', 'every', 'tr', '##ace', 'of', 'her', 'existence', 'has', 'been', 'w', '##ip', '##ed', 'out', 'l', 'r', 's', 'life', 'becomes', 'a', 'tort', '##ured', 'existence', 'as', 'one', 'strange', 'event', 'after', 'another', 'occurs', 'to', 'confir', '##m', 'in', 'his', 'mind', 'that', 'a', 'conspiracy', 'is', 'working', 'against', 'his', 'finding', 'v', '##es', '##pa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l r br ##ane loves his life his car his apartment his job but especially his girlfriend v ##es ##pa one day while show ##ering v ##es ##pa runs out of sh ##amp ##oo l r runs across the street to a conven ##ience store to buy some more a quick trip of no more than a few minutes when he returns v ##es ##pa is gone and every tr ##ace of her existence has been w ##ip ##ed out l r s life becomes a tort ##ured existence as one strange event after another occurs to confir ##m in his mind that a conspiracy is working against his finding v ##es ##pa'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of unknown tokens: 0.0002\n",
      "total number of tokens: 127002\n"
     ]
    }
   ],
   "source": [
    "vocabSize = tokenizerBPE.get_vocab_size()\n",
    "print('size of vocabulary: {}'.format(vocabSize))\n",
    "for i in range(10):\n",
    "    print('vocabulary id: {0}, word: {1}'.format(i, tokenizerBPE.id_to_token(i)))\n",
    "    j = vocabSize-i-1\n",
    "    print('vocabulary id: {0}, word: {1}'.format(j, tokenizerBPE.id_to_token(j)))\n",
    "    \n",
    "display(df_test.loc[0, 'description'])\n",
    "out=tokenizerBPE.encode(df_test.loc[0,'description'])\n",
    "print(out.ids)\n",
    "print(out.tokens)\n",
    "display(tokenizerBPE.decode(out.ids))\n",
    "\n",
    "outFull=tokenizerBPE.encode_batch(df_test['description'])\n",
    "ntokens = 0\n",
    "nunk = 0\n",
    "for encoded in outFull:\n",
    "    ntokens += len(encoded.ids) \n",
    "    nunk += len(encoded.ids) - np.count_nonzero(encoded.ids)\n",
    "print('ratio of unknown tokens: {0:.4f}'.format(nunk/ntokens))\n",
    "print('total number of tokens: {}'.format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0a394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerWP = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "# train tokenizer\n",
    "trainer = WordPieceTrainer(special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]'],\\\n",
    "                           continuing_subword_prefix='##',\\\n",
    "                           vocab_size=10000)\n",
    "tokenizerWP.normalizer = StripAccents() # add method for stripping accents\n",
    "tokenizerWP.pre_tokenizer = Sequence([WhitespaceSplit(), Punctuation(behavior='removed')]) \n",
    "tokenizerWP.post_processor = TemplateProcessing(single='[CLS] $0 [SEP]',\\\n",
    "                                              special_tokens=[('[CLS]',1),('[SEP]',2)])\n",
    "tokenizerWP.train_from_iterator(df_train['description'], trainer=trainer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f00e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 10000\n",
      "vocabulary id: 0, word: [UNK]\n",
      "vocabulary id: 9999, word: marivela\n",
      "vocabulary id: 1, word: [CLS]\n",
      "vocabulary id: 9998, word: marijuana\n",
      "vocabulary id: 2, word: [SEP]\n",
      "vocabulary id: 9997, word: marginal\n",
      "vocabulary id: 3, word: [PAD]\n",
      "vocabulary id: 9996, word: marisa\n",
      "vocabulary id: 4, word: 0\n",
      "vocabulary id: 9995, word: mario\n",
      "vocabulary id: 5, word: 1\n",
      "vocabulary id: 9994, word: marks\n",
      "vocabulary id: 6, word: 2\n",
      "vocabulary id: 9993, word: marine\n",
      "vocabulary id: 7, word: 3\n",
      "vocabulary id: 9992, word: mari\n",
      "vocabulary id: 8, word: 4\n",
      "vocabulary id: 9991, word: whenever\n",
      "vocabulary id: 9, word: 5\n",
      "vocabulary id: 9990, word: allog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"l.r. brane loves his life - his car, his apartment, his job, but especially his girlfriend, vespa. one day while showering, vespa runs out of shampoo. l.r. runs across the street to a convenience store to buy some more, a quick trip of no more than a few minutes. when he returns, vespa is gone and every trace of her existence has been wiped out. l.r.'s life becomes a tortured existence as one strange event after another occurs to confirm in his mind that a conspiracy is working against his finding vespa.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 25, 31, 268, 1112, 1829, 156, 271, 156, 385, 156, 1964, 156, 980, 237, 2555, 156, 1456, 1656, 76, 7316, 298, 395, 587, 419, 1246, 1656, 76, 7316, 3638, 299, 138, 231, 543, 2589, 25, 31, 3638, 1209, 123, 2112, 136, 14, 8021, 740, 4227, 136, 4889, 466, 437, 14, 1831, 1405, 138, 583, 437, 603, 14, 1286, 3349, 294, 169, 1729, 1656, 76, 7316, 151, 3461, 135, 496, 1762, 207, 138, 184, 2538, 255, 574, 7307, 68, 299, 25, 31, 32, 271, 806, 14, 8495, 68, 2538, 189, 298, 2195, 1345, 373, 989, 6727, 136, 6625, 70, 139, 156, 1432, 195, 14, 8720, 151, 1410, 910, 156, 5478, 1656, 76, 7316, 2]\n",
      "['[CLS]', 'l', 'r', 'br', '##ane', 'loves', 'his', 'life', 'his', 'car', 'his', 'apartment', 'his', 'job', 'but', 'especially', 'his', 'girlfriend', 've', '##s', '##pa', 'one', 'day', 'while', 'show', '##ering', 've', '##s', '##pa', 'runs', 'out', 'of', 'sh', '##amp', '##oo', 'l', 'r', 'runs', 'across', 'the', 'street', 'to', 'a', 'conven', '##ience', 'store', 'to', 'buy', 'some', 'more', 'a', 'quick', 'trip', 'of', 'no', 'more', 'than', 'a', 'few', 'minutes', 'when', 'he', 'returns', 've', '##s', '##pa', 'is', 'gone', 'and', 'every', 'tra', '##ce', 'of', 'her', 'existence', 'has', 'been', 'wipe', '##d', 'out', 'l', 'r', 's', 'life', 'becomes', 'a', 'torture', '##d', 'existence', 'as', 'one', 'strange', 'event', 'after', 'another', 'occurs', 'to', 'confir', '##m', 'in', 'his', 'mind', 'that', 'a', 'conspiracy', 'is', 'working', 'against', 'his', 'finding', 've', '##s', '##pa', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l r br ##ane loves his life his car his apartment his job but especially his girlfriend ve ##s ##pa one day while show ##ering ve ##s ##pa runs out of sh ##amp ##oo l r runs across the street to a conven ##ience store to buy some more a quick trip of no more than a few minutes when he returns ve ##s ##pa is gone and every tra ##ce of her existence has been wipe ##d out l r s life becomes a torture ##d existence as one strange event after another occurs to confir ##m in his mind that a conspiracy is working against his finding ve ##s ##pa'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of unknown tokens: 0.0002\n",
      "total number of tokens: 128626\n"
     ]
    }
   ],
   "source": [
    "vocabSize = tokenizerWP.get_vocab_size()\n",
    "print('size of vocabulary: {}'.format(vocabSize))\n",
    "for i in range(10):\n",
    "    print('vocabulary id: {0}, word: {1}'.format(i, tokenizerWP.id_to_token(i)))\n",
    "    j = vocabSize-i-1\n",
    "    print('vocabulary id: {0}, word: {1}'.format(j, tokenizerWP.id_to_token(j)))\n",
    "    \n",
    "display(df_test.loc[0, 'description'])\n",
    "out=tokenizerWP.encode(df_test.loc[0,'description'])\n",
    "print(out.ids)\n",
    "print(out.tokens)\n",
    "display(tokenizerWP.decode(out.ids))\n",
    "\n",
    "outFull=tokenizerWP.encode_batch(df_test['description'])\n",
    "ntokens = 0\n",
    "nunk = 0\n",
    "for encoded in outFull:\n",
    "    ntokens += len(encoded.ids) \n",
    "    nunk += len(encoded.ids) - np.count_nonzero(encoded.ids)\n",
    "print('ratio of unknown tokens: {0:.4f}'.format(nunk/ntokens))\n",
    "print('total number of tokens: {}'.format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14345b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
